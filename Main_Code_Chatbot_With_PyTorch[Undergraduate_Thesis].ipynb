{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/marveltimothyy/Generative-Chatbot-/blob/main/Main_Code_Chatbot_With_PyTorch%5BUndergraduate_Thesis%5D.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ekjg4BOWfbOK",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#Data and Library preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uZCRYUPYfbOK",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##Import Library and init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\ProgramFiles\\Anaconda\\envs\\rinL\\lib\\site-packages\\numpy\\_distributor_init.py:32: UserWarning: loaded more than 1 DLL from .libs:\n",
      "D:\\ProgramFiles\\Anaconda\\envs\\rinL\\lib\\site-packages\\numpy\\.libs\\libopenblas.PYQHXLVVQ7VESDPUVUADXEVJOBGHJPAY.gfortran-win_amd64.dll\n",
      "D:\\ProgramFiles\\Anaconda\\envs\\rinL\\lib\\site-packages\\numpy\\.libs\\libopenblas.XWYDX2IKJW2NMTWSFYNGFUWKQU3LYTCZ.gfortran-win_amd64.dll\n",
      "  stacklevel=1)\n"
     ]
    },
    {
     "data": {
      "text/plain": "<torch._C.Generator at 0x21e09362110>"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lib for Preprocessing and load data \n",
    "import os\n",
    "from io import open\n",
    "import csv\n",
    "import random\n",
    "import re\n",
    "import unicodedata\n",
    "import itertools\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "from Model.ChaoticGRU import ChaoticGRU\n",
    "from Model.ChaoticLSTM import ChaoticLSTM\n",
    "#Lib for Modeling\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#Bleu Evaluation\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "#Setup Cuda\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if USE_CUDA else \"cpu\")\n",
    "torch.manual_seed(1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# import os\n",
    "# from google.colab import drive\n",
    "#\n",
    "# drive.mount('/content/drive', force_remount=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##Load Daset "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import os\n",
    "corpus_name = \"cornell movie-dialogs corpus\"\n",
    "corpus = os.path.join(\"./data\", corpus_name)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "datafile = os.path.join(corpus, \"formatted_movie_lines.txt\")\n",
    "save_dir =  os.path.join(\"./save\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#Text Preprocessing"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Vocabulary "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# Default word tokens\n",
    "PAD_token = 0  # Used for padding short sentences\n",
    "SOS_token = 1  # Start-of-sentence token\n",
    "EOS_token = 2  # End-of-sentence token\n",
    "\n",
    "class Voc:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {PAD_token: \"PAD\", \n",
    "                           SOS_token: \"SOS\",\n",
    "                           EOS_token: \"EOS\"}\n",
    "        self.num_words = 3  # Count SOS, EOS, PAD\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.num_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.num_words] = word\n",
    "            self.num_words += 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Text Preprocessing"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "MAX_LENGTH = 10  # Maximum sentence length to consider\n",
    "class TextPreprocessing:\n",
    "  def unicodeToAscii(s):\n",
    "      return ''.join(\n",
    "          c for c in unicodedata.normalize('NFD', s)\n",
    "          if unicodedata.category(c) != 'Mn')\n",
    "  def normalizeString(s):\n",
    "      s = TextPreprocessing.unicodeToAscii(s.lower().strip())\n",
    "      s = re.sub(r\"[^a-z]+\", r\" \", s)\n",
    "      s = re.sub(r\"\\s+\", r\" \", s).strip()\n",
    "      return s\n",
    "  def readVocs(datafile, corpus_name):\n",
    "      lines = open(datafile, encoding='utf-8').read().strip().split('\\n')\n",
    "      pairs = [[TextPreprocessing.normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
    "      voc = Voc(corpus_name)\n",
    "      return voc, pairs\n",
    "  def filterPair(p):\n",
    "      return (len(p[0].split(' ')) < MAX_LENGTH and len(p[1].split(' ')) < MAX_LENGTH)\n",
    "  def filterPairs(pairs):\n",
    "      return [pair for pair in pairs if TextPreprocessing.filterPair(pair)]\n",
    "  def dropNull(pairs):\n",
    "      return [pair for pair in pairs if pair[0] != '' and pair[1] != '']\n",
    "  def loadPrepareData(corpus_name, datafile):\n",
    "      voc, pairs = TextPreprocessing.readVocs(datafile, corpus_name)\n",
    "      pairs = TextPreprocessing.filterPairs(pairs)\n",
    "      pairs = TextPreprocessing.dropNull(pairs)\n",
    "      for pair in pairs:\n",
    "          voc.addSentence(pair[0])\n",
    "          voc.addSentence(pair[1])\n",
    "      return voc, pairs\n",
    "voc, pairs = TextPreprocessing.loadPrepareData(corpus_name, datafile)\n",
    "# pairs = []\n",
    "for sentences in pairs[:10]:\n",
    "    print(pairs)\n",
    "pairs_train, pairs_test = train_test_split(pairs, test_size=0.1, random_state=42)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#Data adjustment"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "import itertools\n",
    "class DataAdjustment:\n",
    "  def tokenization(voc, sentence):\n",
    "      return [voc.word2index[word] for word in sentence.split(' ')] + [EOS_token]\n",
    "\n",
    "  def zeroPadding(l, fillvalue=PAD_token):\n",
    "      return list(itertools.zip_longest(*l, fillvalue=fillvalue))\n",
    "\n",
    "  def dataVar(l, voc, con = True):\n",
    "      indexes_batch = [DataAdjustment.tokenization(voc, sentence) for sentence in l]\n",
    "      padList = DataAdjustment.zeroPadding(indexes_batch)\n",
    "      padVar = torch.LongTensor(padList)\n",
    "      if con:\n",
    "        lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n",
    "        return padVar, lengths\n",
    "      else:\n",
    "        max_target_len = max([len(indexes) for indexes in indexes_batch])\n",
    "        return padVar, max_target_len\n",
    "\n",
    "  def adjustBatchData(voc, pair_batch):\n",
    "      pair_batch.sort(key=lambda x: len(x[0].split(\" \")), reverse=True)\n",
    "      input_batch, output_batch = [], []\n",
    "      for pair in pair_batch:\n",
    "          input_batch.append(pair[0])\n",
    "          output_batch.append(pair[1])\n",
    "      inp, lengths = DataAdjustment.dataVar(input_batch, voc)\n",
    "      output, max_target_len = DataAdjustment.dataVar(output_batch, voc, False)\n",
    "      return inp, lengths, output, max_target_len\n",
    "\n",
    "  def batching(batch_size, iterable):\n",
    "      args = [iter(iterable)] * batch_size\n",
    "      return ([e for e in t if e != None] for t in itertools.zip_longest(*args))\n",
    "\n",
    "# small_batch_size = 5\n",
    "# batches = DataAdjustment.batch2TrainData(voc, [random.choice(pairs) for _ in \n",
    "#                                 range(small_batch_size)])\n",
    "# input_variable, lengths, target_variable, mask, max_target_len = batches\n",
    "\n",
    "# print(\"input_variable:\", input_variable)\n",
    "# print(\"lengths:\", lengths)\n",
    "# print(\"target_variable:\", target_variable)\n",
    "# print(\"mask:\", mask)\n",
    "# print(\"max_target_len:\", max_target_len)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#Architecure Sequence-to-sequence"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ENCODER"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "class EncoderGRU(nn.Module):\n",
    "    def __init__(self, hidden_size, embedding):\n",
    "        super(EncoderGRU, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = embedding\n",
    "        # self.gru = nn.GRU(hidden_size,\n",
    "        #                   hidden_size,\n",
    "        #                   bidirectional=True)\n",
    "        self.gru = ChaoticGRU(hidden_size, hidden_size, False, True)\n",
    "    def forward(self, input_seq, input_lengths, hidden=None):\n",
    "        embedded = self.embedding(input_seq)\n",
    "        print(\"In: EncoderGRU, embedded\", embedded.size())\n",
    "        # packed = nn.utils.rnn.pack_padded_sequence(embedded, input_lengths)\n",
    "        # outputs, hidden = self.gru(packed, hidden)\n",
    "        outputs, hidden = self.gru(embedded, hidden)\n",
    "        # hidden: [batch_size, 2, hidden_size]\n",
    "        # outputs, _ = nn.utils.rnn.pad_packed_sequence(outputs)\n",
    "        print(\"In: EncoderGRU, outputs_1\", outputs.size())\n",
    "        outputs = outputs[:, :, :self.hidden_size] + outputs[:, :, self.hidden_size:]\n",
    "        print(\"In: EncoderGRU, outputs_2\", outputs.size(), \" hidden: \", hidden.size())\n",
    "        return outputs, hidden"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Decoder + Attention implement"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "class AttnDecoderGRU(nn.Module):\n",
    "    def __init__(self, embedding, hidden_size, \n",
    "                 output_size):\n",
    "        super(AttnDecoderGRU, self).__init__()\n",
    "        #decoder_input, decoder_hidden, encoder_outputs\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        # Define layers\n",
    "        self.embedding = embedding\n",
    "        # self.gru = nn.GRU(hidden_size,\n",
    "        #                   hidden_size,\n",
    "        #                   bidirectional = False)\n",
    "        self.gru = ChaoticGRU(hidden_size, hidden_size, False, False)\n",
    "        self.concat = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, input_step, last_hidden, encoder_outputs):\n",
    "        print(\"In: AttnDecoderGRU, input_step\", input_step.size())\n",
    "        embedded = self.embedding(input_step)\n",
    "        print(\"In: AttnDecoderGRU, embedded\", embedded.size(), \"hidden: \", last_hidden.size())\n",
    "        # Forward through unidirectional GRU\n",
    "        gru_output, hidden = self.gru(embedded, last_hidden)\n",
    "\n",
    "        # Calculate attention weights from the current GRU output\n",
    "        luong_dot_score = torch.sum(gru_output * encoder_outputs, dim=2)\n",
    "        attn_energies = luong_dot_score.t()\n",
    "        attn_weights = F.softmax(attn_energies, dim=1).unsqueeze(1)\n",
    "\n",
    "        context = attn_weights.bmm(encoder_outputs.transpose(0, 1))\n",
    "  \n",
    "        gru_output = gru_output.squeeze(0)\n",
    "        context = context.squeeze(1)\n",
    "        concat_input = torch.cat((gru_output, context), 1)\n",
    "        concat_output = torch.tanh(self.concat(concat_input))\n",
    "\n",
    "        output = self.out(concat_output)\n",
    "        output = F.softmax(output, dim=1)\n",
    "        print(\"In: AttnDecoderGRU, output\", output.size(), \" hidden: \", hidden.size())\n",
    "        return output, hidden"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#Train-Test Step"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train step"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def train(input_variable, lengths, target_variable, max_target_len, \n",
    "          encoder, decoder, embedding, encoder_optimizer, decoder_optimizer, \n",
    "          batch_size, clip, max_length=MAX_LENGTH):\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_variable = input_variable.to(device)\n",
    "    target_variable = target_variable.to(device)\n",
    "    lengths = lengths.to(\"cpu\")\n",
    "\n",
    "    loss = 0\n",
    "    print_losses = []\n",
    "\n",
    "    encoder_outputs, encoder_hidden = encoder(input_variable, lengths)\n",
    "    print(\"In: train, encoder_outputs\", encoder_outputs.size(), \" encoder_hidden: \", encoder_hidden.size())\n",
    "    decoder_input = torch.LongTensor([[SOS_token for _ in range(batch_size)]])\n",
    "    decoder_input = decoder_input.to(device)\n",
    "\n",
    "    # decoder_hidden = encoder_hidden\n",
    "    decoder_hidden = encoder_hidden[:1]\n",
    "    print(\"In: train, decoder_input\", decoder_input.size(), \" decoder_hidden: \", decoder_hidden.size())\n",
    "    # Forward batch of sequences through decoder one time step at a time\n",
    "    for t in range(max_target_len):\n",
    "        decoder_output, decoder_hidden = decoder(\n",
    "            decoder_input, decoder_hidden, encoder_outputs\n",
    "        )\n",
    "        print(\"In: train, decoder_output\", decoder_output.size(), \" decoder_hidden: \", decoder_hidden.size())\n",
    "        # Teacher forcing: next input is current target\n",
    "        decoder_input = target_variable[t].view(1, -1)\n",
    "        # Calculate and accumulate loss\n",
    "        decoder_output = torch.log(decoder_output)\n",
    "        mask_loss = criterion(decoder_output,\n",
    "                                        target_variable[t])\n",
    "        loss += mask_loss\n",
    "        print_losses.append(mask_loss.item())\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    _ = nn.utils.clip_grad_norm_(encoder.parameters(), clip)\n",
    "    _ = nn.utils.clip_grad_norm_(decoder.parameters(), clip)\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return sum(print_losses)/max_target_len"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##Test Step **EVALUATION**"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def test(input_variable, lengths, target_variable, max_target_len, \n",
    "          encoder, decoder, embedding, encoder_optimizer, decoder_optimizer, \n",
    "          batch_size, clip, max_length=MAX_LENGTH):\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_variable = input_variable.to(device)\n",
    "    target_variable = target_variable.to(device)\n",
    "    lengths = lengths.to(\"cpu\")\n",
    "\n",
    "    loss = 0\n",
    "    print_losses = []\n",
    "\n",
    "    encoder_outputs, encoder_hidden = encoder(input_variable, lengths)\n",
    "\n",
    "    decoder_input = torch.LongTensor([[SOS_token for _ in range(batch_size)]])\n",
    "    decoder_input = decoder_input.to(device)\n",
    "\n",
    "    decoder_hidden = encoder_hidden[:1]\n",
    "\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "          for t in range(max_target_len):\n",
    "              decoder_output, decoder_hidden = decoder(\n",
    "                  decoder_input, decoder_hidden, encoder_outputs\n",
    "              )\n",
    "              _, topi = decoder_output.topk(1)\n",
    "              decoder_input = torch.LongTensor(\n",
    "                  [[topi[i][0] for i in range(batch_size)]])\n",
    "              decoder_input = decoder_input.to(device)\n",
    "              decoder_output = torch.log(decoder_output)\n",
    "              mask_loss = criterion(decoder_output,\n",
    "                                              target_variable[t])\n",
    "              loss += mask_loss\n",
    "              print_losses.append(mask_loss.item())\n",
    "\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "    return sum(print_losses) / max_target_len"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "---\n",
    "#Result\n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##Save State for (1536-128-15-0.0001)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building encoder and decoder ...\n",
      "Building optimizers ...\n",
      "Epoch 1\n",
      "In: EncoderGRU, embedded torch.Size([10, 128, 1536])\n",
      "torch.Size([128, 1536])\n",
      "torch.Size([128, 1536])\n",
      "torch.Size([128, 1536])\n",
      "torch.Size([128, 1536])\n",
      "torch.Size([128, 1536])\n",
      "torch.Size([128, 1536])\n",
      "torch.Size([128, 1536])\n",
      "torch.Size([128, 1536])\n",
      "torch.Size([128, 1536])\n",
      "torch.Size([128, 1536])\n",
      "In: EncoderGRU, outputs_1 torch.Size([10, 128, 3072])\n",
      "In: EncoderGRU, outputs_2 torch.Size([10, 128, 1536])  hidden:  torch.Size([2, 128, 1536])\n",
      "In: train, encoder_outputs torch.Size([10, 128, 1536])  encoder_hidden:  torch.Size([2, 128, 1536])\n",
      "In: train, decoder_input torch.Size([1, 128])  decoder_hidden:  torch.Size([1, 128, 1536])\n",
      "In: AttnDecoderGRU, input_step torch.Size([1, 128])\n",
      "In: AttnDecoderGRU, embedded torch.Size([1, 128, 1536]) hidden:  torch.Size([1, 128, 1536])\n",
      "torch.Size([1, 128, 1536])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (3072) must match the size of tensor b (1536) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_28324\\1167506394.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     51\u001B[0m             loss = train(input_variable, lengths, target_variable, \n\u001B[0;32m     52\u001B[0m                         \u001B[0mmax_target_len\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mencoder\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdecoder\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0membedding\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 53\u001B[1;33m                         encoder_optimizer, decoder_optimizer, batch_size, clip)\n\u001B[0m\u001B[0;32m     54\u001B[0m             \u001B[0mprint_loss\u001B[0m \u001B[1;33m+=\u001B[0m \u001B[0mloss\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     55\u001B[0m             \u001B[0mloss_list\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mprint_loss\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_28324\\2416926488.py\u001B[0m in \u001B[0;36mtrain\u001B[1;34m(input_variable, lengths, target_variable, max_target_len, encoder, decoder, embedding, encoder_optimizer, decoder_optimizer, batch_size, clip, max_length)\u001B[0m\n\u001B[0;32m     24\u001B[0m     \u001B[1;32mfor\u001B[0m \u001B[0mt\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmax_target_len\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     25\u001B[0m         decoder_output, decoder_hidden = decoder(\n\u001B[1;32m---> 26\u001B[1;33m             \u001B[0mdecoder_input\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdecoder_hidden\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mencoder_outputs\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     27\u001B[0m         )\n\u001B[0;32m     28\u001B[0m         \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"In: train, decoder_output\"\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdecoder_output\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msize\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m\" decoder_hidden: \"\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdecoder_hidden\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msize\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\ProgramFiles\\Anaconda\\envs\\rinL\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1128\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[0;32m   1129\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[1;32m-> 1130\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1131\u001B[0m         \u001B[1;31m# Do not call functions when jit is used\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1132\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_28324\\3000381727.py\u001B[0m in \u001B[0;36mforward\u001B[1;34m(self, input_step, last_hidden, encoder_outputs)\u001B[0m\n\u001B[0;32m     20\u001B[0m         \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"In: AttnDecoderGRU, embedded\"\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0membedded\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msize\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m\"hidden: \"\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mlast_hidden\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msize\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     21\u001B[0m         \u001B[1;31m# Forward through unidirectional GRU\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 22\u001B[1;33m         \u001B[0mgru_output\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mhidden\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mgru\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0membedded\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mlast_hidden\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     23\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     24\u001B[0m         \u001B[1;31m# Calculate attention weights from the current GRU output\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\ProgramFiles\\Anaconda\\envs\\rinL\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001B[0m in \u001B[0;36m_call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1128\u001B[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001B[0;32m   1129\u001B[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001B[1;32m-> 1130\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0mforward_call\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0minput\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1131\u001B[0m         \u001B[1;31m# Do not call functions when jit is used\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1132\u001B[0m         \u001B[0mfull_backward_hooks\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mnon_full_backward_hooks\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m[\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32mD:\\Git Workspace\\Generative-Chatbot\\Model\\ChaoticGRU.py\u001B[0m in \u001B[0;36mforward\u001B[1;34m(self, x, initStates)\u001B[0m\n\u001B[0;32m    111\u001B[0m                     \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msigmoid\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mgates\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mhiddenSize\u001B[0m\u001B[1;33m:\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mhiddenSize\u001B[0m \u001B[1;33m*\u001B[0m \u001B[1;36m2\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mto\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdevice\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    112\u001B[0m                 )\n\u001B[1;32m--> 113\u001B[1;33m                 \u001B[0mnt\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtanh\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mxt\u001B[0m \u001B[1;33m@\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mWni\u001B[0m \u001B[1;33m+\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mBni\u001B[0m \u001B[1;33m+\u001B[0m \u001B[0mrt\u001B[0m \u001B[1;33m*\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0mht\u001B[0m \u001B[1;33m@\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mWnh\u001B[0m \u001B[1;33m+\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mBnh\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mto\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mx\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdevice\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    114\u001B[0m                 \u001B[1;31m# Compute the hidden.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    115\u001B[0m                 \u001B[0mht\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;33m(\u001B[0m\u001B[1;36m1\u001B[0m \u001B[1;33m-\u001B[0m \u001B[0mzt\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m*\u001B[0m \u001B[0mnt\u001B[0m \u001B[1;33m+\u001B[0m \u001B[0mzt\u001B[0m \u001B[1;33m*\u001B[0m \u001B[0mht\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mRuntimeError\u001B[0m: The size of tensor a (3072) must match the size of tensor b (1536) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "hidden_size = 1536\n",
    "batch_size = 128\n",
    "epoch = 15\n",
    "learning_rate = 0.0001\n",
    "clip = 50.0\n",
    "teacher_forcing_ratio = 1.0\n",
    "loss_list = [] \n",
    "start_iteration = 1\n",
    "print_loss = 0\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "print('Building encoder and decoder ...')\n",
    "# Initialize word embeddings\n",
    "embedding = nn.Embedding(voc.num_words, hidden_size)\n",
    "\n",
    "# Initialize encoder & decoder models\n",
    "encoder = EncoderGRU(hidden_size, embedding)\n",
    "decoder = AttnDecoderGRU(embedding, \n",
    "                         hidden_size, \n",
    "                         voc.num_words)\n",
    "# Use appropriate device\n",
    "encoder = encoder.to(device)\n",
    "decoder = decoder.to(device)\n",
    "\n",
    "encoder.train()\n",
    "decoder.train()\n",
    "\n",
    "# Initialize optimizers\n",
    "print('Building optimizers ...')\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "\n",
    "criterion = nn.NLLLoss(ignore_index=PAD_token, reduction='mean')\n",
    "\n",
    "train_batch = list(DataAdjustment.batching(batch_size, pairs))\n",
    "training_batches = [DataAdjustment.adjustBatchData(voc, \n",
    "                                    [train_batch[j][i] \n",
    "                                    for i in range(len(train_batch[j]))])\n",
    "                  for j in range(len(train_batch)-1)]\n",
    "\n",
    "for ep in range(epoch):\n",
    "  random.shuffle(training_batches)\n",
    "  print(\"Epoch {}\".format(ep+1))\n",
    "  #data train session \n",
    "  for iteration in range(start_iteration, len(train_batch)-1):\n",
    "            training_batch = training_batches[iteration - 1]\n",
    "\n",
    "            input_variable, lengths, target_variable, max_target_len = training_batch\n",
    "            \n",
    "            loss = train(input_variable, lengths, target_variable, \n",
    "                        max_target_len, encoder, decoder, embedding, \n",
    "                        encoder_optimizer, decoder_optimizer, batch_size, clip)\n",
    "            print_loss += loss\n",
    "            loss_list=print_loss\n",
    "            print(\"\\tIteration: {}; Percent complete: {:.1f}%; Average loss: {:.4f}\".format(iteration, iteration/(len(train_batch)-1) * 100, print_loss))\n",
    "            print_loss = 0\n",
    "\n",
    "# Save checkpoint\n",
    "# directory = os.path.join(save_dir)\n",
    "# if not os.path.exists(directory):\n",
    "#     os.makedirs(directory)\n",
    "# torch.save({\n",
    "#     'en': encoder.state_dict(),\n",
    "#     'de': decoder.state_dict(),\n",
    "#     'en_opt': encoder_optimizer.state_dict(),\n",
    "#     'de_opt': decoder_optimizer.state_dict(),\n",
    "#     'voc_dict': voc.__dict__,\n",
    "#     'embedding': embedding.state_dict()\n",
    "# }, os.path.join(directory, '3{}-{}-{}-{}.tar'.format(hidden_size, batch_size, epoch, learning_rate)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KvEtFG6xehMQ",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class DecoderPredict(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(DecoderPredict, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, input_seq, input_length, max_length):\n",
    "        encoder_outputs, encoder_hidden = self.encoder(input_seq, input_length)\n",
    "        print(\"In: DecoderPredict, encoder\", encoder_outputs.size())\n",
    "        decoder_hidden = encoder_hidden[:1]\n",
    "        decoder_input = torch.ones(1, 1, device=device,\n",
    "                                   dtype=torch.long) * SOS_token\n",
    "        all_tokens = torch.zeros([0], device=device, dtype=torch.long)\n",
    "        all_scores = torch.zeros([0], device=device)\n",
    "        for _ in range(max_length):\n",
    "            decoder_output, decoder_hidden = self.decoder(decoder_input,\n",
    "                                                          decoder_hidden,\n",
    "                                                          encoder_outputs)\n",
    "            decoder_scores, decoder_input = torch.max(decoder_output, dim=1)\n",
    "            all_tokens = torch.cat((all_tokens, decoder_input), dim=0)\n",
    "            all_scores = torch.cat((all_scores, decoder_scores), dim=0)\n",
    "            decoder_input = torch.unsqueeze(decoder_input, 0)\n",
    "        return all_tokens, all_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OUFcwB60ehMR",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "encoder.eval()\n",
    "decoder.eval()\n",
    "searcher = DecoderPredict(encoder, decoder)\n",
    "class InputProcessing:\n",
    "  def evaluate(encoder, decoder, searcher, voc, sentence, max_length=MAX_LENGTH):\n",
    "    indexes_batch = [DataAdjustment.tokenization(voc, sentence)]\n",
    "    lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n",
    "    input_batch = torch.LongTensor(indexes_batch).transpose(0, 1)\n",
    "    input_batch = input_batch.to(device)\n",
    "    lengths = lengths.to(\"cpu\")\n",
    "    tokens, scores = searcher(input_batch, lengths, max_length)\n",
    "    decoded_words = [voc.index2word[token.item()] for token in tokens]\n",
    "    return decoded_words\n",
    "  def response_only(input_sentence):\n",
    "      input_sentence = TextPreprocessing.normalizeString(input_sentence)\n",
    "      output_words = InputProcessing.evaluate(encoder, decoder, searcher, voc, input_sentence)\n",
    "      outword = []\n",
    "      for i in output_words:\n",
    "        if i == 'EOS' or i =='PAD':\n",
    "          break\n",
    "        else:\n",
    "          outword.append(i)\n",
    "      return ' '.join(outword)"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def set_response():\n",
    "    input_list = ['can play music?','Hello','how are you?','can you smile?', 'good morning', 'what is your name?', 'are you okay?', 'thanks', 'can you help me?', 'do you love me?', 'what are you doing?', 'i love you', 'good night', 'bye']\n",
    "    out_dict= {} \n",
    "    for i in input_list:\n",
    "\n",
    "      input_sentence = TextPreprocessing.normalizeString(i)\n",
    "      output_words = InputProcessing.evaluate(encoder, decoder, searcher, voc, input_sentence)\n",
    "\n",
    "      outword = []\n",
    "      for j in output_words:\n",
    "        if j == 'EOS':\n",
    "          break\n",
    "        elif j != 'PAD':\n",
    "\n",
    "          outword.append(j)\n",
    "      string = ' '.join(outword)\n",
    "      string = re.sub(' ll ', \"'ll \",string)\n",
    "      string = re.sub(' t ', \"'t \",string)\n",
    "      string = re.sub(' d ', \"'d \",string)\n",
    "      string = re.sub(' re ', \"'re \",string)\n",
    "      string = re.sub(' s ', \"'s \",string)\n",
    "      string = re.sub(' m ', \" am \",string)\n",
    "      string = re.sub(' ve ', \"'ve \",string)\n",
    "      out_dict[i] = string\n",
    "\n",
    "    for j in input_list:\n",
    "      print(\"Human :\", j)\n",
    "      print(\"Bot   :\", out_dict[j])\n",
    "set_response()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rM3XOWWSlRRS",
    "outputId": "724215dc-49c0-4999-e056-074fe790bef2",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "step must be greater than zero",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_28324\\1473601210.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[0mencoder_hidden\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mrand\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m10\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m128\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;36m1536\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 2\u001B[1;33m \u001B[0mdecoder_hidden\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mencoder_hidden\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m-\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      3\u001B[0m \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdecoder_hidden\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msize\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mValueError\u001B[0m: step must be greater than zero"
     ]
    }
   ],
   "source": [
    "encoder_hidden = torch.rand([10, 128, 1536])\n",
    "decoder_hidden = encoder_hidden[::-1]\n",
    "print(decoder_hidden.size())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 2, 1536])\n"
     ]
    }
   ],
   "source": [
    "ht = torch.rand([128, 1536])\n",
    "hinvt = torch.rand([128, 1536])\n",
    "ht = torch.stack([ht, hinvt], dim=1)\n",
    "print(ht.size())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 10, 1536])\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand([128, 1536])\n",
    "list = []\n",
    "for x in range(10):\n",
    "    list.append(a.unsqueeze(1))\n",
    "# print(list)\n",
    "output = torch.cat(list, dim = 1)\n",
    "print(output.size())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "OzgX6e39fbOL",
    "Kjrgcl_TdGin",
    "sQmJlZbcfbOP",
    "cvne-TOZfbOQ",
    "dD1uYyPIsXY0"
   ],
   "name": " Main Code Chatbot With PyTorch[Undergraduate Thesis].ipynb",
   "provenance": [],
   "toc_visible": true,
   "include_colab_link": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}